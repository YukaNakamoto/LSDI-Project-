{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API Analysis\n",
    "\n",
    "![alt text](../images/image.png \"Title\")\n",
    "This configuration shows the hourly day-ahead (price of energy until the same time tomorrow) for the last two weeks.\n",
    "When checking the network traffic for the above dates and for the hourly resolution, you will find three .json files being fetched from the API.\n",
    "\n",
    "A request to the api has the following structure:\n",
    "https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_[timestamp_in_milliseconds].json\n",
    "\n",
    "The following request fetch data for the corresponding time frames.\n",
    "\n",
    "https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_1729461600000.json:\n",
    "Sunday, 6 October 2024 22:00:00 -> Sunday, 13 October 2024 21:00:00\n",
    "\n",
    "https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_1728856800000.json:\n",
    "Sunday, 13 October 2024 22:00:00 -> Sunday, 20 October 2024 21:00:00\n",
    "\n",
    "https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_1729461600000.json\n",
    "Sunday, 20 October 2024 22:00:00 -> Sunday, 27 October 2024 22:00:00\n",
    "\n",
    "\n",
    "You will find that for example the timestamp 1729461600000 maps to the initial date Sunday, 6 October 2024 22:00:00 and every file contains the date for one week. Interestingly enough the site only shows the data for two weeks even though it had to fetch the data for three entire weeks. If the above links are broken, it may be due to a shift in daylight savings time (DST) which we will have to take into account.\n",
    "\n",
    "Additionally you will see that each .json file contains around 172 (more or less) time series entries for an entire week.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the scraper\n",
    "We now want to implement a scraper that fetches the hourly energy prices for n amount of days. With the above information we now know that we'll have to find the corresponding timestamps for each week and to fetch the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO) \n",
    "\n",
    "logger = logging.getLogger(\"scraper_logger\")\n",
    "\n",
    "# console_handler = logging.StreamHandler()\n",
    "file_handler = logging.FileHandler(\"app.log\")\n",
    "\n",
    "# console_handler.setLevel(logging.WARNING)\n",
    "file_handler.setLevel(logging.WARNING) \n",
    "\n",
    "# logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape(url, delay):\n",
    "    response =  requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    time.sleep(delay)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Define Berlin timezone\n",
    "tz_berlin = pytz.timezone(\"Europe/Berlin\")\n",
    "\n",
    "# Calculate last Monday in Berlin time, taking into account local DST\n",
    "now = datetime.now(tz_berlin)\n",
    "days_since_monday = now.weekday()\n",
    "last_monday_berlin = now - timedelta(days=days_since_monday)\n",
    "last_monday_berlin = last_monday_berlin.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Convert Berlin time to UTC and get the timestamp in milliseconds\n",
    "last_monday_utc = last_monday_berlin.astimezone(pytz.UTC)\n",
    "last_monday_utc_ms = int(last_monday_utc.timestamp() * 1000)\n",
    "\n",
    "print(\"Berlin time (local):\", last_monday_berlin)\n",
    "print(\"UTC time:\", last_monday_utc)\n",
    "print(\"UTC timestamp (ms):\", last_monday_utc_ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "# Define Berlin timezone\n",
    "tz_berlin = pytz.timezone(\"Europe/Berlin\")\n",
    "\n",
    "# Calculate last Monday in Berlin time, taking into account local DST\n",
    "now = datetime.now(tz_berlin)\n",
    "days_since_monday = now.weekday()\n",
    "last_monday_berlin = now - timedelta(days=days_since_monday)\n",
    "last_monday_berlin = last_monday_berlin.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Convert Berlin time to UTC and get the timestamp in milliseconds\n",
    "last_monday_utc = last_monday_berlin.astimezone(pytz.UTC)\n",
    "last_monday_utc_ms = int(last_monday_utc.timestamp() * 1000)\n",
    "\n",
    "print(\"Berlin time (local):\", last_monday_berlin)\n",
    "print(\"UTC time:\", last_monday_utc)\n",
    "print(\"UTC timestamp (ms):\", last_monday_utc_ms)\n",
    "\n",
    "# Define constants\n",
    "week_in_ms = 24 * 60 * 60 * 1000 * 7\n",
    "delay = 0.5  # seconds\n",
    "n = 500  # number of weeks\n",
    "base_url = \"https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_{}.json\"\n",
    "\n",
    "# Use a dictionary to store unique timestamps and prices\n",
    "energy_ts_data = {}\n",
    "\n",
    "for k in range(n):\n",
    "    last_monday_berlin = last_monday_utc.astimezone(tz_berlin)\n",
    "    last_monday_utc = last_monday_berlin.astimezone(pytz.UTC)\n",
    "    last_monday_utc_ms = int(last_monday_utc.timestamp() * 1000)\n",
    "\n",
    "    # Adjust timestamp for daylight savings time (berlin tz) if necessary\n",
    "    if last_monday_berlin.dst() != timedelta(0):  # DST is in effect\n",
    "        last_monday_utc_ms -= 60 * 60 * 1000\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url.format(last_monday_utc_ms))\n",
    "        response.raise_for_status()\n",
    "        logging.info(f\"Successfully scraped data for ts: {last_monday_berlin} (Europe/Berlin)\")\n",
    "        json_data = response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logging.warning(f\"Failed to scrape data for timestamp: {last_monday_utc} (UTC)\\n\\tError: {http_err}\")\n",
    "        continue\n",
    "    except requests.exceptions.JSONDecodeError as decoder_error:\n",
    "        logging.warning(f\"Failed to deserialize JSON: \\n\\tError: {decoder_error}\")\n",
    "        continue\n",
    "\n",
    "    # Parse the JSON response\n",
    "    parsed_json = dict(json_data)\n",
    "\n",
    "    for ts, price in parsed_json.get(\"series\", []):\n",
    "        try:\n",
    "            price_float = float(price)\n",
    "            # Convert to naive timestamp\n",
    "            ts_datetime = datetime.fromtimestamp(ts / 1000).replace(tzinfo=None).isoformat()\n",
    "            print(ts_datetime)\n",
    "            # Add to the dictionary, overwriting any duplicates\n",
    "            energy_ts_data[ts_datetime] = price_float\n",
    "        except TypeError as e:\n",
    "            logging.warning(f\"Failed to parse non-float value for timestamp {ts}\\n\\tError: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Move to the previous week\n",
    "    last_monday_utc = last_monday_utc - timedelta(weeks=1)\n",
    "\n",
    "# Convert the dictionary to a sorted list of tuples\n",
    "energy_ts_data_sorted = sorted(energy_ts_data.items())\n",
    "\n",
    "# Convert to a NumPy array\n",
    "data = np.array(energy_ts_data_sorted)\n",
    "\n",
    "print(\"Final dataset shape:\", data.shape)\n",
    "\n",
    "# Save the data as a CSV file (naive timestamps only)\n",
    "np.savetxt(\"../data/day_ahead_energy_prices.csv\", data, delimiter=\",\", fmt=\"%s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weather:\n",
    "-- wind\n",
    "-- sun \n",
    "-- temp\n",
    "\n",
    "- per day energy mix\n",
    "- gas price per day\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime.now()\n",
    "end_date = datetime(2018, 9, 30)\n",
    "delta = timedelta(days=1)\n",
    "delay = 0.2\n",
    "\n",
    "# end_date = start_date - (10 * delta)\n",
    "\n",
    "base_url = \"https://www.energy-charts.info/charts/energy_pie/data/de/day_pie_{}.json\"\n",
    "\n",
    "current_date = start_date\n",
    "res = []\n",
    "while current_date >= end_date:\n",
    "    try:\n",
    "        cd_format = current_date.strftime(\"%Y_%m_%d\")\n",
    "        response = scrape(base_url.format(cd_format), delay)\n",
    "\n",
    "        logging.info(f\"Successfully scraped data for date: {cd_format}\")\n",
    "        res.append((cd_format, response.json()))\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logging.warning(f\"Failed to scrape data for date: {cd_format} (UTC)\\n\\tError: {http_err}\")\n",
    "    except requests.exceptions.JSONDecodeError as decoder_error:\n",
    "        logging.warning(f\"Failed to deserialize JSON: \\n\\tError: {decoder_error}\")\n",
    "    current_date -= delta\n",
    "\n",
    "\n",
    "print(len(res))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Mix Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cross_boarder_e_trading = True\n",
    "cbet = \"Cross border electricity trading\"\n",
    "\n",
    "dtype = [('date', 'U50'), ('e_component', 'U50'), ('value', 'float32')]\n",
    "\n",
    "# Initialize an empty structured array\n",
    "array = np.empty(0, dtype=dtype)\n",
    "\n",
    "for date, data in res:\n",
    "    sources = []\n",
    "    for e_source in data:\n",
    "        name = str(e_source[\"name\"][\"en\"])\n",
    "\n",
    "        if exclude_cross_boarder_e_trading and name == cbet:\n",
    "            continue\n",
    "\n",
    "        # Ensure numeric conversion or default to 0\n",
    "        try:\n",
    "            y_value = float(e_source[\"y\"])\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "        \n",
    "        sources.append((date, name, y_value))\n",
    "    \n",
    "    # Convert to a structured array with the correct dtype\n",
    "    arr = np.array(sources, dtype=dtype)\n",
    "    \n",
    "    # Normalize the 'value' column\n",
    "    arr['value'] /= np.sum(arr['value'], axis=0)\n",
    "\n",
    "    # Append to the main array\n",
    "    array = np.append(array, arr)\n",
    "\n",
    "np.savetxt(\"../data/daily_market_mix.csv\", array, delimiter=\",\", fmt=\"%s\")\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST /api/raw-data HTTP/1.1\n",
    "Content-Type: application/json\n",
    "Accept: */*\n",
    "Sec-Fetch-Site: cross-site\n",
    "Accept-Language: en-GB,en;q=0.9\n",
    "Accept-Encoding: gzip, deflate, br\n",
    "Sec-Fetch-Mode: cors\n",
    "Origin: https://www.agora-energiewende.de\n",
    "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.0.1 Safari/605.1.15\n",
    "Content-Length: 538\n",
    "Referer: https://www.agora-energiewende.de/\n",
    "Connection: keep-alive\n",
    "Sec-Fetch-Dest: empty\n",
    "X-Requested-With: XMLHttpRequest\n",
    "Api-key: agora_live_62ce76dd202927.67115829\n",
    "Priority: u=3, i\n",
    "\n",
    "\n",
    "{\"filters\":{\"from\":\"2023-11-01\",\"to\":\"2024-10-01\",\"generation\":[\"Total electricity demand\",\"Biomass\",\"Hydro\",\"Wind offshore\",\"Wind onshore\",\"Solar\",\"Total conventional power plant\",\"Nuclear\",\"Lignite\",\"Hard Coal\",\"Natural Gas\",\"Pumped storage generation\",\"Other\",\"Grid emission factor\",\"Total grid emissions\",\"Total Renewables\",\"Total Conventional\",\"Renewable share\",\"Conventional share\"]},\"x_coordinate\":\"date_id\",\"y_coordinate\":\"value\",\"view_name\":\"live_gen_plus_emi_de_hourly\",\"kpi_name\":\"power_generation\",\"z_coordinate\":\"generation\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint and headers\n",
    "url = \"https://api.agora-energy.org/api/raw-data\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Sec-Fetch-Site\": \"cross-site\",\n",
    "    \"Accept-Language\": \"en-GB,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Sec-Fetch-Mode\": \"cors\",\n",
    "    \"Origin\": \"https://www.agora-energiewende.de\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.0.1 Safari/605.1.15\",\n",
    "    \"Referer\": \"https://www.agora-energiewende.de/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Sec-Fetch-Dest\": \"empty\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"Api-key\": \"agora_live_62ce76dd202927.67115829\",\n",
    "}\n",
    "\n",
    "out = []\n",
    "\n",
    "# Define the payload\n",
    "for year in [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    payload = {\n",
    "        \"filters\": {\n",
    "            \"from\": f\"{year}-10-01\",\n",
    "            \"to\": f\"{year + 1}-09-30\",\n",
    "            \"generation\": [\n",
    "                \"Total electricity demand\", \"Biomass\", \"Hydro\", \"Wind offshore\",\n",
    "                \"Wind onshore\", \"Solar\", \"Total conventional power plant\", \"Nuclear\",\n",
    "                \"Lignite\", \"Hard Coal\", \"Natural Gas\", \"Pumped storage generation\",\n",
    "                \"Other\", \"Grid emission factor\", \"Total grid emissions\", \"Total Renewables\",\n",
    "                \"Total Conventional\", \"Renewable share\", \"Conventional share\"\n",
    "            ]\n",
    "        },\n",
    "        \"x_coordinate\": \"date_id\",\n",
    "        \"y_coordinate\": \"value\",\n",
    "        \"view_name\": \"live_gen_plus_emi_de_hourly\",\n",
    "        \"kpi_name\": \"power_generation\",\n",
    "        \"z_coordinate\": \"generation\"\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Request was successful!\", year, year+1)\n",
    "        data = {}\n",
    "        data = response.json()\n",
    "        out.extend(data[\"data\"][\"data\"])\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\", year, year+1)\n",
    "    time.sleep(0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../data/hourly_market_mix.csv\", np.array(out), delimiter=\",\", fmt=\"%s\")\n",
    "data = np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Delta' '-0.06395637084779374' '-0.07241054432886612' ...\n",
      " '0.10873861726297138' '0.09192833825586821' '0.09788072545944428']\n"
     ]
    }
   ],
   "source": [
    "#\"../data/hourly_market_mix.csv\" :\n",
    "# 2017-10-01T00:00:00,5.276,Biomass\n",
    "# 2017-10-01T00:00:00,445.727,Grid emission factor\n",
    "# 2017-10-01T00:00:00,3.738,Hard Coal\n",
    "# 2017-10-01T00:00:00,2.19,Hydro\n",
    "# 2017-10-01T00:00:00,15.846,Lignite\n",
    "# 2017-10-01T00:00:00,6.634,Natural Gas\n",
    "# 2017-10-01T00:00:00,10.19,Nuclear\n",
    "# 2017-10-01T00:00:00,1.763,Other\n",
    "# 2017-10-01T00:00:00,0.235,Pumped storage generation\n",
    "# 2017-10-01T00:00:00,0.0,Solar\n",
    "# 2017-10-01T00:00:00,38.406,Total conventional power plant\n",
    "# 2017-10-01T00:00:00,49.965,Total electricity demand\n",
    "# 2017-10-01T00:00:00,24676.021,Total grid emissions\n",
    "# 2017-10-01T00:00:00,1.048,Wind offshore\n",
    "# 2017-10-01T00:00:00,8.442,Wind onshore\n",
    "# 2017-10-01T01:00:00,5.269,Biomass\n",
    "# 2017-10-01T01:00:00,450.157,Grid emission factor\n",
    "# 2017-10-01T01:00:00,3.5,Hard Coal\n",
    "# 2017-10-01T01:00:00,2.202,Hydro\n",
    "# 2017-10-01T01:00:00,15.843,Lignite\n",
    "# 2017-10-01T01:00:00,6.752,Natural Gas\n",
    "# 2017-10-01T01:00:00,10.307,Nuclear\n",
    "# 2017-10-01T01:00:00,1.788,Other\n",
    "# 2017-10-01T01:00:00,0.187,Pumped storage generation\n",
    "# 2017-10-01T01:00:00,0.0,Solar\n",
    "# 2017-10-01T01:00:00,38.377,Total conventional power plant\n",
    "# 2017-10-01T01:00:00,49.062,Total electricity demand\n",
    "# 2017-10-01T01:00:00,24548.501,Total grid emissions\n",
    "# 2017-10-01T01:00:00,0.907,Wind offshore\n",
    "# 2017-10-01T01:00:00,7.778,Wind onshore\n",
    "\n",
    "\n",
    "\n",
    "mix_rows = []\n",
    "delta_abs_mix_rows = []\n",
    "delta_mix_rows = []\n",
    "other_metrics_rows = []\n",
    "\n",
    "energy_by_hour = np.loadtxt(\"../data/hourly_market_mix.csv\", delimiter=\",\", dtype=str)\n",
    "\n",
    "\n",
    "# Defining Categories\n",
    "mix_categories = [\n",
    "    \"Biomass\",\n",
    "    \"Hard Coal\",\n",
    "    \"Hydro\",\n",
    "    \"Lignite\",\n",
    "    \"Natural Gas\",\n",
    "    \"Nuclear\",\n",
    "    \"Other\",\n",
    "    \"Pumped storage generation\",\n",
    "    \"Solar\",\n",
    "    \"Wind offshore\",\n",
    "    \"Wind onshore\",\n",
    "]\n",
    "\n",
    "other_metrics = [\n",
    "    \"Grid emission factor\",\n",
    "    \"Total conventional power plant\",\n",
    "    \"Total electricity demand\",\n",
    "    \"Total grid emissions\",\n",
    "]\n",
    "\n",
    "# Define start and end dates as naive datetime objects\n",
    "start_date = datetime.fromisoformat(\"2018-10-01T00:00:00\")\n",
    "end_date = datetime.fromisoformat(\"2024-10-30T00:00:00\")\n",
    "\n",
    "# Generate list of hourly timestamps betweens start and end date\n",
    "timestamps = [\n",
    "    start_date + timedelta(hours=i)\n",
    "    for i in range(int((end_date - start_date).total_seconds() // 3600) + 1)\n",
    "]\n",
    "\n",
    "# Converts the timestamps to ISO format\n",
    "timestamp_strings = [ts.isoformat() for ts in timestamps]\n",
    "\n",
    "# create  dict of timestamps as keys with empty lists\n",
    "data_dict = {ts: [] for ts in timestamp_strings}\n",
    "\n",
    "# add each energy type to dict after iso conversion\n",
    "for d in energy_by_hour:\n",
    "    d_timestamp = datetime.fromisoformat(d[0]).isoformat()  # Naive datetime conversion\n",
    "    if d_timestamp in data_dict:\n",
    "        data_dict[d_timestamp].append(d)\n",
    "\n",
    "\n",
    "for ts in timestamp_strings:\n",
    "    \n",
    "    hour_data = np.array(data_dict.get(ts, []))  # Fetch data for this timestamp\n",
    "    \n",
    "    if hour_data.size == 0:\n",
    "        continue\n",
    "\n",
    "    mix_per_hour = hour_data[np.isin(hour_data[:, 2], mix_categories)]\n",
    "    \n",
    "    if mix_per_hour.size == 0:\n",
    "        continue\n",
    "\n",
    "    if \"Nuclear\" not in mix_per_hour[:, 2]:\n",
    "        mix_per_hour = np.insert(mix_per_hour, 6, [ts, \"0.0\", \"Nuclear\"], axis=0)\n",
    "\n",
    "\n",
    "    # fill None values with 0\n",
    "    mix_per_hour = np.where(mix_per_hour == \"None\", 0.0, mix_per_hour) \n",
    "\n",
    "    total_demand = hour_data[np.isin(hour_data[:, 2], [\"Total electricity demand\"])][0][1] # shorten\n",
    "    \n",
    "    total_demand = total_demand.astype(float)\n",
    "    if total_demand != \"None\":\n",
    "\n",
    "        # add delta to new dataset\n",
    "        mix_per_hour_delta = np.copy(mix_per_hour)\n",
    "\n",
    "        # get delta between total energy demand and produced energy in mix\n",
    "        delta = total_demand - mix_per_hour[:, 1].astype(float).sum() \n",
    "\n",
    "        mix_per_hour_delta = np.insert(mix_per_hour_delta, mix_per_hour_delta.shape[0], [ts, delta, \"delta\"], axis=0)\n",
    "\n",
    "        # retain absolute delta dataset\n",
    "        mix_per_hour_delta_abs = np.copy(mix_per_hour_delta)\n",
    "\n",
    "        # create percentages relative to energy demanded from delta dataset\n",
    "        mix_per_hour_delta[:, 1] = (\n",
    "            mix_per_hour_delta[:, 1].astype(float) / total_demand\n",
    "        )\n",
    "\n",
    "        mix_per_hour_delta_abs_row = np.concatenate(([ts], mix_per_hour_delta_abs[:, 1]))\n",
    "        delta_abs_mix_rows.append(mix_per_hour_delta_abs_row)\n",
    "        \n",
    "        mix_per_hour_delta_row = np.concatenate(([ts], mix_per_hour_delta[:, 1]))\n",
    "        delta_mix_rows.append(mix_per_hour_delta_row)\n",
    "\n",
    "\n",
    "\n",
    "    # create percentages relative to produced energy from original dataset\n",
    "    mix_per_hour[:, 1] = (\n",
    "        mix_per_hour[:, 1].astype(float) / mix_per_hour[:, 1].astype(float).sum()\n",
    "    )\n",
    "    \n",
    "    mix_per_hour_row = np.concatenate(([ts], mix_per_hour[:, 1]))\n",
    "    mix_rows.append(mix_per_hour_row)\n",
    "\n",
    "    other_metrics_per_hour = hour_data[np.isin(hour_data[:, 2], other_metrics)]\n",
    "    if other_metrics_per_hour.size == 0:\n",
    "        continue\n",
    "    row = np.concatenate(([ts], other_metrics_per_hour[:, 1]))\n",
    "    \n",
    "    other_metrics_rows.append(row)\n",
    "\n",
    "    \n",
    "percentage_mix = np.vstack([[\"Timestamp\"] + mix_categories] + mix_rows)\n",
    "mix_categories.append(\"Delta\")\n",
    "delta_abs_mix = np.vstack([[\"Timestamp\"] + mix_categories] + delta_abs_mix_rows)\n",
    "delta_percentage_mix = np.vstack([[\"Timestamp\"] + mix_categories] + delta_mix_rows)\n",
    "percentage_sources = np.vstack([[\"Timestamp\"] + other_metrics] + other_metrics_rows)\n",
    "\n",
    "np.savetxt(\"../data/hourly_market_mix_cleaned.csv\", percentage_mix, delimiter=\",\", fmt=\"%s\")\n",
    "np.savetxt(\"../data/hourly_market_mix_delta.csv\", delta_percentage_mix, delimiter=\",\", fmt=\"%s\")\n",
    "np.savetxt(\"../data/hourly_market_mix_delta_abs.csv\", delta_abs_mix, delimiter=\",\", fmt=\"%s\")\n",
    "np.savetxt(\"../data/hourly_market_metrics_cleaned.csv\", percentage_sources, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "import importlib.util\n",
    "\n",
    "# Function to check if required packages are installed\n",
    "def check_package_installed(package_name):\n",
    "    package_spec = importlib.util.find_spec(package_name)\n",
    "    if package_spec is None:\n",
    "        print(f\"{package_name} is not installed!\")\n",
    "    else:\n",
    "        print(f\"{package_name} is installed!\")\n",
    "\n",
    "# Mapping of module names to package names\n",
    "packages = {\n",
    "    \"openmeteo_requests\": \"openmeteo-requests\",\n",
    "    \"requests_cache\": \"requests-cache\",\n",
    "    \"retry_requests\": \"retry-requests\",\n",
    "}\n",
    "\n",
    "for module_name, package_name in packages.items():\n",
    "    check_package_installed(module_name)\n",
    "\n",
    "# Setup the Open-Meteo API client with caching and retries\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Define the base URL for the weather API\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Define the list of representative coordinates for Germany\n",
    "coordinates = [\n",
    "{\"latitude\": 52.5200, \"longitude\": 13.4050},  # Berlin\n",
    "{\"latitude\": 48.1351, \"longitude\": 11.5820},  # Munich\n",
    "{\"latitude\": 50.1109, \"longitude\": 8.6821},   # Frankfurt\n",
    "{\"latitude\": 51.1657, \"longitude\": 10.4515},  # Central Germany (approximate)\n",
    "{\"latitude\": 53.5511, \"longitude\": 9.9937},   # Hamburg\n",
    "{\"latitude\": 51.2277, \"longitude\": 6.7735},   # Düsseldorf\n",
    "{\"latitude\": 51.0504, \"longitude\": 13.7373},  # Dresden\n",
    "{\"latitude\": 50.9375, \"longitude\": 6.9603},   # Cologne\n",
    "{\"latitude\": 49.4875, \"longitude\": 8.4660},   # Mannheim\n",
    "{\"latitude\": 48.7758, \"longitude\": 9.1829},   # Stuttgart\n",
    "{\"latitude\": 51.3397, \"longitude\": 12.3731},  # Leipzig\n",
    "{\"latitude\": 50.0782, \"longitude\": 8.2398},   # Wiesbaden\n",
    "{\"latitude\": 49.0069, \"longitude\": 8.4037},   # Karlsruhe\n",
    "{\"latitude\": 51.5128, \"longitude\": 7.4633},   # Dortmund\n",
    "{\"latitude\": 50.1211, \"longitude\": 8.4965},   # Offenbach\n",
    "{\"latitude\": 50.3569, \"longitude\": 7.5886},   # Koblenz\n",
    "{\"latitude\": 50.7753, \"longitude\": 6.0839},   # Aachen\n",
    "{\"latitude\": 49.4521, \"longitude\": 11.0767},  # Nuremberg\n",
    "{\"latitude\": 52.3759, \"longitude\": 9.7320},   # Hanover\n",
    "{\"latitude\": 51.4818, \"longitude\": 7.2162},   # Bochum\n",
    "{\"latitude\": 51.4556, \"longitude\": 7.0116},   # Essen\n",
    "{\"latitude\": 51.4344, \"longitude\": 6.7623},   # Duisburg\n",
    "{\"latitude\": 51.9607, \"longitude\": 7.6261},   # Münster\n",
    "\n",
    "]\n",
    "\n",
    "# Define the weather variables and date range\n",
    "params_template = {\n",
    "    \"start_date\": \"2018-01-01\",\n",
    "    \"end_date\": \"2024-11-21\",\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\",\n",
    "        #\"relative_humidity_2m\",\n",
    "         \"precipitation\",\n",
    "       # \"surface_pressure\", \"cloud_cover\",\n",
    "        \"wind_speed_100m\",\n",
    "        #\"sunshine_duration\",\n",
    "        \"shortwave_radiation\", \"direct_radiation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Store data for all locations\n",
    "all_data = []\n",
    "\n",
    "for coord in coordinates:\n",
    "    params = params_template.copy()\n",
    "    params.update({\n",
    "        \"latitude\": coord[\"latitude\"],\n",
    "        \"longitude\": coord[\"longitude\"],\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        # Fetch weather data for the current location\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "\n",
    "        # Extract hourly data for this location\n",
    "        hourly = response.Hourly()\n",
    "        hourly_data = {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\"\n",
    "            )\n",
    "        }\n",
    "        hourly_data[\"temperature_2m\"] = hourly.Variables(0).ValuesAsNumpy()\n",
    "      #  hourly_data[\"relative_humidity_2m\"] = hourly.Variables(1).ValuesAsNumpy()\n",
    "        hourly_data[\"precipitation\"] = hourly.Variables(1).ValuesAsNumpy()\n",
    "       # hourly_data[\"surface_pressure\"] = hourly.Variables(3).ValuesAsNumpy()\n",
    "        #hourly_data[\"cloud_cover\"] = hourly.Variables(4).ValuesAsNumpy()\n",
    "        hourly_data[\"wind_speed_100m\"] = hourly.Variables(2).ValuesAsNumpy()\n",
    "       # hourly_data[\"sunshine_duration\"] = hourly.Variables(6).ValuesAsNumpy()\n",
    "      #  hourly_data[\"shortwave_radiation\"] = hourly.Variables(3).ValuesAsNumpy()\n",
    "        hourly_data[\"direct_radiation\"] = hourly.Variables(3).ValuesAsNumpy()\n",
    "\n",
    "        # Convert to DataFrame and append to the list\n",
    "        hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "        all_data.append(hourly_dataframe)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for coordinates {coord}: {e}\")\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "combined_df = pd.concat(all_data)\n",
    "\n",
    "# Group by date and calculate the mean for all variables\n",
    "averaged_data = combined_df.groupby(\"date\").mean()\n",
    "\n",
    "# Rename columns for better understanding\n",
    "averaged_data = averaged_data.rename(columns={\n",
    "   # \"shortwave_radiation\": \"Global Horizontal Irradiance\",\n",
    "    \"precipitation\": \"Precipitation (rain/snow)\"\n",
    "})\n",
    "\n",
    "# Save the averaged data to a CSV file\n",
    "csv_file = \"../data/germany_weather_average.csv\"\n",
    "averaged_data.to_csv(csv_file, index=True)\n",
    "\n",
    "print(f\"Averaged weather data for Germany saved to {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read CSV File\n",
    "csv_file = \"../data/germany_weather_average.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# filter all columns including numbers\n",
    "numeric_columns = data.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "# Find all columns with negativ values\n",
    "negative_columns = [col for col in numeric_columns if (data[col] < 0).any()]\n",
    "\n",
    "# Print all Headers with negativ values\n",
    "print(\"Columns with negative values:\", negative_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../data/germany_weather_average.csv\")\n",
    "\n",
    "non_numeric_columns = data.select_dtypes(exclude=[\"number\"]).columns\n",
    "\n",
    "print(\"Headers with non-numeric values:\", non_numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T18:40:33.104512Z",
     "start_time": "2024-12-18T18:40:19.140400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average wind speed saved to ../data/weighted_windspeed.csv.\n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "\n",
    "# Setup the Open-Meteo API client with caching and retries\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Define the base URL for the weather API\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Offshore wind parks (coordinates and weights)\n",
    "wind_parks = [\n",
    "    {\"latitude\": 54.008333, \"longitude\": 6.598333, \"weight\": 60},      # Alpha Ventus\n",
    "    {\"latitude\": 54.358333, \"longitude\": 5.975, \"weight\": 400},        # BARD Offshore I\n",
    "    {\"latitude\": 53.690, \"longitude\": 6.480, \"weight\": 113.4},         # Riffgat\n",
    "    {\"latitude\": 54.15, \"longitude\": 7.25, \"weight\": 295},             # Amrumbank West\n",
    "    {\"latitude\": 54.53, \"longitude\": 6.25, \"weight\": 200},             # Butendiek\n",
    "    {\"latitude\": 54.367, \"longitude\": 6.467, \"weight\": 295},           # DanTysk\n",
    "    {\"latitude\": 54.480, \"longitude\": 7.370, \"weight\": 288},           # Meerwind Süd|Ost\n",
    "    {\"latitude\": 54.4, \"longitude\": 6.6, \"weight\": 576},               # Gode Wind 1 & 2\n",
    "    {\"latitude\": 54.30, \"longitude\": 6.65, \"weight\": 400},             # Global Tech I\n",
    "    {\"latitude\": 53.88, \"longitude\": 6.59, \"weight\": 450},             # Borkum Riffgrund 1\n",
    "    {\"latitude\": 53.88, \"longitude\": 6.59, \"weight\": 215},             # Borkum Riffgrund 2\n",
    "    {\"latitude\": 54.00, \"longitude\": 6.58, \"weight\": 342},             # Trianel Windpark Borkum\n",
    "    {\"latitude\": 54.22, \"longitude\": 6.63, \"weight\": 332},             # Nordsee Ost\n",
    "    {\"latitude\": 54.25, \"longitude\": 7.25, \"weight\": 385},             # Hohe See\n",
    "    {\"latitude\": 54.28, \"longitude\": 7.30, \"weight\": 252},             # Albatros\n",
    "    {\"latitude\": 54.48, \"longitude\": 6.78, \"weight\": 350},             # Wikinger\n",
    "    {\"latitude\": 54.55, \"longitude\": 6.37, \"weight\": 402},             # Arkona\n",
    "    {\"latitude\": 54.45, \"longitude\": 6.58, \"weight\": 600},             # Veja Mate\n",
    "    {\"latitude\": 54.33, \"longitude\": 7.18, \"weight\": 300},             # Deutsche Bucht\n",
    "    {\"latitude\": 54.25, \"longitude\": 7.18, \"weight\": 402},             # Kaskasi\n",
    "    {\"latitude\": 53.610278, \"longitude\": 7.429167, \"weight\": 318.2},  # Windpark Holtriem-Dornum\n",
    "    {\"latitude\": 53.973889, \"longitude\": 8.933333, \"weight\": 302.45},  # Windpark Friedrichskoog\n",
    "    {\"latitude\": 54.611111, \"longitude\": 8.903611, \"weight\": 293.4},  # Bürgerwindpark Reußenköge\n",
    "{\"latitude\": 53.338333, \"longitude\": 13.764444, \"weight\": 242.5},  # Windfeld Uckermark\n",
    "{\"latitude\": 53.715278, \"longitude\": 13.319722, \"weight\": 202.85},  # RH2-Werder/Kessin/Altentreptow\n",
    "{\"latitude\": 51.131667, \"longitude\": 11.964167, \"weight\": 188.1},  # Windpark Stößen-Teuchern\n",
    "{\"latitude\": 52.539722, \"longitude\": 12.871667, \"weight\": 175.2},  # Windpark Ketzin\n",
    "{\"latitude\": 52.515833, \"longitude\": 11.780833, \"weight\": 151.3},  # Windpark Hüselitz\n",
    "{\"latitude\": 51.031667, \"longitude\": 10.629722, \"weight\": 152.25},  # Windfeld Wangenheim-Hochheim-Ballstädt-Westhausen\n",
    "{\"latitude\": 52.354722, \"longitude\": 14.373056, \"weight\": 133.9},  # Windpark Odervorland\n",
    "{\"latitude\": 51.640278, \"longitude\": 8.912222, \"weight\": 129.445},  # Windpark Asseln\n",
    "{\"latitude\": 52.001389, \"longitude\": 12.830833, \"weight\": 128.2},  # Windpark Feldheim\n",
    "{\"latitude\": 51.395556, \"longitude\": 11.709167, \"weight\": 122.1},  # Windpark Esperstedt-Obhausen\n",
    "{\"latitude\": 51.960833, \"longitude\": 11.606389, \"weight\": 114.45},  # Windpark Biere-Borne\n",
    "{\"latitude\": 53.3375, \"longitude\": 7.095833, \"weight\": 106.25},  # Windpark Wybelsumer Polder\n",
    "{\"latitude\": 53.388056, \"longitude\": 7.377778, \"weight\": 102.34},  # Windpark Ihlow\n",
    "{\"latitude\": 52.015556, \"longitude\": 13.193333, \"weight\": 98.8},  # Windpark Heidehof\n",
    "{\"latitude\": 51.546389, \"longitude\": 13.868611, \"weight\": 93.1},  # Windpark Klettwitz\n",
    "{\"latitude\": 52.662778, \"longitude\": 11.709167, \"weight\": 93.5},  # Windpark Schinne-Grassau\n",
    "{\"latitude\": 51.989722, \"longitude\": 10.833333, \"weight\": 92.4},  # Windpark Druiberg\n",
    "{\"latitude\": 51.579722, \"longitude\": 11.708611, \"weight\": 89.3},  # Windpark Beesenstedt-Rottelsdorf\n",
    "{\"latitude\": 52.123333, \"longitude\": 11.160000, \"weight\": 87.65},  # Windpark Ausleben-Badeleben-Wormsdorf\n",
    "{\"latitude\": 53.070833, \"longitude\": 7.739167, \"weight\": 86.5},  # Windpark Saterland\n",
    "{\"latitude\": 51.721111, \"longitude\": 11.644167, \"weight\": 83.35},  # Windpark Alsleben\n",
    "{\"latitude\": 51.798611, \"longitude\": 11.491944, \"weight\": 83.05},  # Windpark Blaue Warthe\n",
    "{\"latitude\": 51.474167, \"longitude\": 13.249722, \"weight\": 82.8},  # Windfeld Randowhöhe\n",
    "{\"latitude\": 51.173056, \"longitude\": 11.350556, \"weight\": 82.65},  # Windpark Rastenberg-Olbersleben\n",
    "{\"latitude\": 51.975833, \"longitude\": 11.451944, \"weight\": 79.1},  # Windpark Egeln-Nord\n",
    "{\"latitude\": 53.363056, \"longitude\": 7.705000, \"weight\": 77.4},  # Windpark Wiesmoor\n",
    "{\"latitude\": 51.774444, \"longitude\": 12.700833, \"weight\": 77.1},  # Windpark Dorna-Kemberg-Schnellin\n",
    "{\"latitude\": 52.027778, \"longitude\": 11.367778, \"weight\": 76.9},  # Windfeld Sonnenberg\n",
    "{\"latitude\": 53.320833, \"longitude\": 12.026944, \"weight\": 75.2},  # Windpark Jännersdorf\n",
    "{\"latitude\": 51.617222, \"longitude\": 8.803333, \"weight\": 75.05},  # Windpark Altenautal\n",
    "{\"latitude\": 52.192500, \"longitude\": 11.368056, \"weight\": 71.3},  # Windpark Bornstedt-Nordgermersleben-Rottmersleben-Schackensleben\n",
    "{\"latitude\": 51.642500, \"longitude\": 11.658333, \"weight\": 72},  # Windpark Gerbstedt-Ihlewitz\n",
    "{\"latitude\": 49.964722, \"longitude\": 7.652500, \"weight\": 70.5},  # Hunsrück-Windpark Ellern\n",
    "{\"latitude\": 52.867500, \"longitude\": 7.138889, \"weight\": 70.1},  # Windpark Haren\n",
    "{\"latitude\": 51.041111, \"longitude\": 6.530000, \"weight\": 67.2},  # Windpark Königshovener Höhe\n",
    "{\"latitude\": 51.445278, \"longitude\": 8.696944, \"weight\": 65.95},  # Windpark Madfeld-Bleiwäsche\n",
    "{\"latitude\": 53.817778, \"longitude\": 8.078889, \"weight\": 65.6},  # Windpark Altenbruch\n",
    "{\"latitude\": 52.176389, \"longitude\": 11.300000, \"weight\": 64.1},  # Windpark Hakenstedt\n",
    "{\"latitude\": 51.946111, \"longitude\": 14.462778, \"weight\": 64},  # Windpark Cottbuser Halde\n",
    "{\"latitude\": 51.707778, \"longitude\": 12.239167, \"weight\": 62.7},  # Windpark Thurland\n",
    "{\"latitude\": 49.689167, \"longitude\": 8.106944, \"weight\": 61},  # Windfeld Rheinhessen-Pfalz\n",
    "{\"latitude\": 50.003333, \"longitude\": 7.386667, \"weight\": 59.8},  # Windpark Kirchberg im Faas\n",
    "{\"latitude\": 51.040556, \"longitude\": 11.620278, \"weight\": 59.1},  # Windpark Eckolstädt\n",
    "{\"latitude\": 51.247500, \"longitude\": 10.283889, \"weight\": 58.1},  # Windpark Büttstedt\n",
    "{\"latitude\": 51.072778, \"longitude\": 11.789444, \"weight\": 58},  # Windpark Molau-Leislau\n",
    "{\"latitude\": 54.483333, \"longitude\": 11.110000, \"weight\": 57.5},  # Windpark Fehmarn-Mitte\n",
    "{\"latitude\": 49.830000, \"longitude\": 8.138889, \"weight\": 55},  # Windpark Wörrstadt\n",
    "{\"latitude\": 49.296111, \"longitude\": 9.415556, \"weight\": 54.9},  # Windpark Harthäuser Wald\n",
    "{\"latitude\": 53.373333, \"longitude\": 9.496944, \"weight\": 52.9},  # Windpark Ahrenswohlde-Wohnste\n",
    "{\"latitude\": 48.980833, \"longitude\": 11.102500, \"weight\": 52.8},  # Windpark Raitenbucher Forst\n",
    "{\"latitude\": 48.740000, \"longitude\": 9.889722, \"weight\": 52.25},  # Windpark Lauterstein\n",
    "{\"latitude\": 49.721111, \"longitude\": 7.721944, \"weight\": 52.2},  # Windpark Lettweiler Höhe\n",
    "{\"latitude\": 50.603056, \"longitude\": 9.243889, \"weight\": 49.65},  # Windpark Goldner Steinrück\n",
    "{\"latitude\": 50.516944, \"longitude\": 6.373611, \"weight\": 49.45},  # Windpark Schleiden-Schöneseiffen\n",
    "{\"latitude\": 53.538889, \"longitude\": 8.952778, \"weight\": 48.8},  # Windpark Köhlen\n",
    "{\"latitude\": 49.764167, \"longitude\": 8.059722, \"weight\": 47.1},  # Windpark Heimersheim\n",
    "{\"latitude\": 53.396667, \"longitude\": 14.169167, \"weight\": 46.3},  # Windfeld Wolfsmoor\n",
    "{\"latitude\": 53.684167, \"longitude\": 8.646111, \"weight\": 46},  # Windpark Holßel\n",
    "{\"latitude\": 51.838333, \"longitude\": 12.875278, \"weight\": 44.9},  # Windpark Elster\n",
    "{\"latitude\": 52.002222, \"longitude\": 12.123056, \"weight\": 44.4},  # Windpark Zerbst\n",
    "{\"latitude\": 52.178333, \"longitude\": 11.886111, \"weight\": 43.6},  # Windpark Stegelitz-Ziepel-Tryppehna\n",
    "{\"latitude\": 53.606944, \"longitude\": 8.793056, \"weight\": 43.2},  # Windpark Kührstedt-Alfstedt\n",
    "{\"latitude\": 52.060111, \"longitude\": 14.381000, \"weight\": 43.2},  # Windpark Ullersdorf\n",
    "{\"latitude\": 49.813333, \"longitude\": 8.017778, \"weight\": 42.4},  # Windpark Gau-Bickelheim\n",
    "{\"latitude\": 51.422778, \"longitude\": 11.834444, \"weight\": 42},  # Windpark Holleben-Bad Lauchstädt\n",
    "{\"latitude\": 54.648611, \"longitude\": 9.176389, \"weight\": 41.8},  # Bürgerwindpark Löwenstedt\n",
    "{\"latitude\": 50.623861, \"longitude\": 9.153528, \"weight\": 41.6},  # Windpark Feldatal\n",
    "{\"latitude\": 51.413056, \"longitude\": 11.587222, \"weight\": 41},  # Windpark Farnstädt\n",
    "{\"latitude\": 52.976667, \"longitude\": 7.415833, \"weight\": 40.9},  # Windpark Dörpen-Ost\n",
    "{\"latitude\": 52.878056, \"longitude\": 10.042778, \"weight\": 40.5},  # Windpark Hermannsburg\n",
    "    {\"latitude\": 52.900000, \"longitude\": 12.384167, \"weight\": 40.4},  # Windpark Kyritz-Plänitz-Zernitz\n",
    "    {\"latitude\": 52.597222, \"longitude\": 12.266667, \"weight\": 40},  # Windpark Stüdenitz\n",
    "]\n",
    "\n",
    "\n",
    "startdate = \"2018-01-01\"\n",
    "enddate = \"2024-11-21\"\n",
    "\n",
    "# Define the weather variable and date range\n",
    "params_template = {\n",
    "    \"start_date\": startdate,\n",
    "    \"end_date\": enddate,\n",
    "    \"hourly\": [\"wind_speed_100m\"]\n",
    "}\n",
    "\n",
    "# Store data for all locations\n",
    "weighted_wind_speed = []\n",
    "\n",
    "for park in wind_parks:\n",
    "    params = params_template.copy()\n",
    "    params.update({\n",
    "        \"latitude\": park[\"latitude\"],\n",
    "        \"longitude\": park[\"longitude\"],\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        # Fetch wind speed data for the current location\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "\n",
    "        # Extract hourly wind speed data for this location\n",
    "        hourly = response.Hourly()\n",
    "        hourly_data = {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\"\n",
    "            ),\n",
    "            \"wind_speed_100m\": hourly.Variables(0).ValuesAsNumpy()\n",
    "        }\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "\n",
    "        # Group by date and calculate the mean wind speed\n",
    "        daily_avg = hourly_dataframe.groupby(\"date\")[\"wind_speed_100m\"].mean()\n",
    "\n",
    "        # Weight the daily averages and append to the list\n",
    "        weighted_wind_speed.append(daily_avg * park[\"weight\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for wind park {park}: {e}\")\n",
    "\n",
    "# Combine weighted wind speeds across all parks\n",
    "total_weight = sum(park[\"weight\"] for park in wind_parks)\n",
    "combined_wind_speed = sum(weighted_wind_speed) / total_weight\n",
    "\n",
    "# Save the weighted average wind speed to a CSV file\n",
    "csv_file = \"../data/weighted_windspeed.csv\"\n",
    "combined_wind_speed.to_csv(csv_file, index=True, header=[\"windspeed 100m\"])\n",
    "\n",
    "print(f\"Weighted average wind speed saved to {csv_file}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T18:47:26.224102Z",
     "start_time": "2024-12-18T18:47:24.914033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        38.490368\n",
      "1        39.208380\n",
      "2        39.968450\n",
      "3        41.720287\n",
      "4        43.592230\n",
      "           ...    \n",
      "60403    16.334919\n",
      "60404    13.902251\n",
      "60405    14.273662\n",
      "60406    14.276556\n",
      "60407    16.874899\n",
      "Name: wind_speed_100m, Length: 60408, dtype: float64\n",
      "0        52.056625\n",
      "1        54.795845\n",
      "2        61.359924\n",
      "3        65.063110\n",
      "4        58.501553\n",
      "           ...    \n",
      "60403    19.196121\n",
      "60404    14.000850\n",
      "60405    10.822694\n",
      "60406     6.807420\n",
      "60407     8.352756\n",
      "Name: windspeed 100m, Length: 60408, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_orig = pd.read_csv('../data/germany_weather_average.csv')\n",
    "df_replacemet = pd.read_csv('../data/weighted_windspeed.csv')\n",
    "\n",
    "#replace column 1 from df_orig with column2 from df_replacement\n",
    "df_orig['wind_speed_100m'] = df_replacemet['windspeed 100m']\n",
    "\n",
    "#save new formatted csv\n",
    "df_orig.to_csv('../data/germany_weather_average.csv', index=False)\n",
    "\n",
    "#print(df_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "\n",
    "# Setup the Open-Meteo API client with caching and retries\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Define the base URL for the weather API\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Define the weather variable and date range\n",
    "params_template = {\n",
    "    \"start_date\": startdate,\n",
    "    \"end_date\": enddate,\n",
    "    \"hourly\": [\"direct_radiation\"]\n",
    "}\n",
    "\n",
    "# Store data for all locations\n",
    "weighted_sun = {}\n",
    "\n",
    "for park in sun_parks:\n",
    "    params = params_template.copy()\n",
    "    params.update({\n",
    "        \"latitude\": park[\"latitude\"],\n",
    "        \"longitude\": park[\"longitude\"],\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        # Fetch wind speed data for the current location\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "\n",
    "        # Extract hourly wind speed data for this location\n",
    "        hourly = response.Hourly()\n",
    "        hourly_data = {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\"\n",
    "            ),\n",
    "            \"direct_radiation\": hourly.Variables(1).ValuesAsNumpy()      }\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "        all_data.append(hourly_dataframe)\n",
    "\n",
    "        daily_avg = hourly_dataframe.groupby(\"date\")[\"direct_radiation\"].mean()\n",
    "\n",
    "\n",
    "        # Weight the daily averages and append to the list\n",
    "        all_data.append(daily_avg * park[\"weight\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for wind park {park}: {e}\")\n",
    "\n",
    "# Combine weighted wind speeds across all parks\n",
    "total_weight = sum(park[\"weight\"] for park in wind_parks)\n",
    "combined_wind_speed = sum(weighted_wind_speed) / total_weight\n",
    "\n",
    "# Save the weighted average wind speed to a CSV file\n",
    "csv_file = \"../data/weighted_windspeed.csv\"\n",
    "combined_wind_speed.to_csv(csv_file, index=True, header=[\"windspeed 100m\"])\n",
    "\n",
    "print(f\"Weighted average wind speed saved to {csv_file}.\")\n",
    "\n",
    "\n",
    "# Combine weighted wind speeds across all parks\n",
    "total_weight = sum(park[\"weight\"] for park in wind_parks)\n",
    "combined_wind_speed = sum(weighted_wind_speed) / total_weight\n",
    "\n",
    "# Save the weighted average wind speed to a CSV file\n",
    "csv_file = \"../data/weighted_windspeed.csv\"\n",
    "combined_wind_speed.to_csv(csv_file, index=True, header=[\"windspeed 100m\"])\n",
    "\n",
    "print(f\"Weighted average wind speed saved to {csv_file}.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_orig = pd.read_csv('../data/germany_weather_average.csv')\n",
    "df_replacemet_sun = pd.read_csv('../data/solar_park_weather_average.csv')\n",
    "df_replacemet_wind = pd.read_csv('../data/weighted_windspeed.csv')\n",
    "\n",
    "\n",
    "#replace column 1 from df_orig with column2 from df_replacement\n",
    "df_orig['direct_radiation'] = df_replacemet_sun['direct_radiation']\n",
    "df_orig['wind_speed_100m'] = df_replacemet_wind['windspeed 100m']\n",
    "\n",
    "\n",
    "#save new formatted csv\n",
    "df_orig.to_csv('../data/germany_weather_average.csv', index=False)\n",
    "\n",
    "#print(df_orig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
