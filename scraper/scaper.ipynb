{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### API Analysis\n",
    "\n",
    "![alt text](../images/image.png \"Title\")\n",
    "This configuration shows the hourly day-ahead (price of energy until the same time tomorrow) for the last two weeks.\n",
    "When checking the network traffic for the above dates and for the hourly resolution, you will find three .json files being fetched from the API.\n",
    "\n",
    "A request to the api has the following structure:\n",
    "https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_[timestamp_in_milliseconds].json\n",
    "\n",
    "The following request fetch data for the corresponding time frames.\n",
    "\n",
    "https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_1729461600000.json:\n",
    "Sunday, 6 October 2024 22:00:00 -> Sunday, 13 October 2024 21:00:00\n",
    "\n",
    "https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_1728856800000.json:\n",
    "Sunday, 13 October 2024 22:00:00 -> Sunday, 20 October 2024 21:00:00\n",
    "\n",
    "https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_1729461600000.json\n",
    "Sunday, 20 October 2024 22:00:00 -> Sunday, 27 October 2024 22:00:00\n",
    "\n",
    "\n",
    "You will find that for example the timestamp 1729461600000 maps to the initial date Sunday, 6 October 2024 22:00:00 and every file contains the date for one week. Interestingly enough the site only shows the data for two weeks even though it had to fetch the data for three entire weeks. If the above links are broken, it may be due to a shift in daylight savings time (DST) which we will have to take into account.\n",
    "\n",
    "Additionally you will see that each .json file contains around 172 (more or less) time series entries for an entire week.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Implementing the scraper\n",
    "We now want to implement a scraper that fetches the hourly energy prices for n amount of days. With the above information we now know that we'll have to find the corresponding timestamps for each week and to fetch the data."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import logging\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pytz\n",
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "logging.basicConfig(level=logging.INFO) \n",
    "\n",
    "logger = logging.getLogger(\"scraper_logger\")\n",
    "\n",
    "# console_handler = logging.StreamHandler()\n",
    "file_handler = logging.FileHandler(\"app.log\")\n",
    "\n",
    "# console_handler.setLevel(logging.WARNING)\n",
    "file_handler.setLevel(logging.WARNING) \n",
    "\n",
    "# logger.addHandler(console_handler)\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def scrape(url, delay):\n",
    "    response =  requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    time.sleep(delay)\n",
    "    return response"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "\n",
    "# Define Berlin timezone\n",
    "tz_berlin = pytz.timezone(\"Europe/Berlin\")\n",
    "\n",
    "# Calculate last Monday in Berlin time, taking into account local DST\n",
    "now = datetime.now(tz_berlin)\n",
    "days_since_monday = now.weekday()\n",
    "last_monday_berlin = now - timedelta(days=days_since_monday)\n",
    "last_monday_berlin = last_monday_berlin.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Convert Berlin time to UTC and get the timestamp in milliseconds\n",
    "last_monday_utc = last_monday_berlin.astimezone(pytz.UTC)\n",
    "last_monday_utc_ms = int(last_monday_utc.timestamp() * 1000)\n",
    "\n",
    "print(\"Berlin time (local):\", last_monday_berlin)\n",
    "print(\"UTC time:\", last_monday_utc)\n",
    "print(\"UTC timestamp (ms):\", last_monday_utc_ms)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import pytz\n",
    "\n",
    "# Define Berlin timezone\n",
    "tz_berlin = pytz.timezone(\"Europe/Berlin\")\n",
    "\n",
    "# Calculate last Monday in Berlin time, taking into account local DST\n",
    "now = datetime.now(tz_berlin)\n",
    "days_since_monday = now.weekday()\n",
    "last_monday_berlin = now - timedelta(days=days_since_monday)\n",
    "last_monday_berlin = last_monday_berlin.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "\n",
    "# Convert Berlin time to UTC and get the timestamp in milliseconds\n",
    "last_monday_utc = last_monday_berlin.astimezone(pytz.UTC)\n",
    "last_monday_utc_ms = int(last_monday_utc.timestamp() * 1000)\n",
    "\n",
    "print(\"Berlin time (local):\", last_monday_berlin)\n",
    "print(\"UTC time:\", last_monday_utc)\n",
    "print(\"UTC timestamp (ms):\", last_monday_utc_ms)\n",
    "\n",
    "# Define constants\n",
    "week_in_ms = 24 * 60 * 60 * 1000 * 7\n",
    "delay = 0.5  # seconds\n",
    "n = 500  # number of weeks\n",
    "base_url = \"https://www.smard.de/app/chart_data/4169/DE/4169_DE_hour_{}.json\"\n",
    "\n",
    "# Use a dictionary to store unique timestamps and prices\n",
    "energy_ts_data = {}\n",
    "\n",
    "for k in range(n):\n",
    "    last_monday_berlin = last_monday_utc.astimezone(tz_berlin)\n",
    "    last_monday_utc = last_monday_berlin.astimezone(pytz.UTC)\n",
    "    last_monday_utc_ms = int(last_monday_utc.timestamp() * 1000)\n",
    "\n",
    "    # Adjust timestamp for daylight savings time (berlin tz) if necessary\n",
    "    if last_monday_berlin.dst() != timedelta(0):  # DST is in effect\n",
    "        last_monday_utc_ms -= 60 * 60 * 1000\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url.format(last_monday_utc_ms))\n",
    "        response.raise_for_status()\n",
    "        logging.info(f\"Successfully scraped data for ts: {last_monday_berlin} (Europe/Berlin)\")\n",
    "        json_data = response.json()\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logging.warning(f\"Failed to scrape data for timestamp: {last_monday_utc} (UTC)\\n\\tError: {http_err}\")\n",
    "        continue\n",
    "    except requests.exceptions.JSONDecodeError as decoder_error:\n",
    "        logging.warning(f\"Failed to deserialize JSON: \\n\\tError: {decoder_error}\")\n",
    "        continue\n",
    "\n",
    "    # Parse the JSON response\n",
    "    parsed_json = dict(json_data)\n",
    "\n",
    "    for ts, price in parsed_json.get(\"series\", []):\n",
    "        try:\n",
    "            price_float = float(price)\n",
    "            # Convert to naive timestamp\n",
    "            ts_datetime = datetime.fromtimestamp(ts / 1000).replace(tzinfo=None).isoformat()\n",
    "            print(ts_datetime)\n",
    "            # Add to the dictionary, overwriting any duplicates\n",
    "            energy_ts_data[ts_datetime] = price_float\n",
    "        except TypeError as e:\n",
    "            logging.warning(f\"Failed to parse non-float value for timestamp {ts}\\n\\tError: {e}\")\n",
    "            continue\n",
    "\n",
    "    # Move to the previous week\n",
    "    last_monday_utc = last_monday_utc - timedelta(weeks=1)\n",
    "\n",
    "# Convert the dictionary to a sorted list of tuples\n",
    "energy_ts_data_sorted = sorted(energy_ts_data.items())\n",
    "\n",
    "# Convert to a NumPy array\n",
    "data = np.array(energy_ts_data_sorted)\n",
    "\n",
    "print(\"Final dataset shape:\", data.shape)\n",
    "\n",
    "# Save the data as a CSV file (naive timestamps only)\n",
    "np.savetxt(\"../data/day_ahead_energy_prices.csv\", data, delimiter=\",\", fmt=\"%s\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- Weather:\n",
    "-- wind\n",
    "-- sun \n",
    "-- temp\n",
    "\n",
    "- per day energy mix\n",
    "- gas price per day\n",
    "- "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "start_date = datetime.now()\n",
    "end_date = datetime(2018, 9, 30)\n",
    "delta = timedelta(days=1)\n",
    "delay = 0.2\n",
    "\n",
    "# end_date = start_date - (10 * delta)\n",
    "\n",
    "base_url = \"https://www.energy-charts.info/charts/energy_pie/data/de/day_pie_{}.json\"\n",
    "\n",
    "current_date = start_date\n",
    "res = []\n",
    "while current_date >= end_date:\n",
    "    try:\n",
    "        cd_format = current_date.strftime(\"%Y_%m_%d\")\n",
    "        response = scrape(base_url.format(cd_format), delay)\n",
    "\n",
    "        logging.info(f\"Successfully scraped data for date: {cd_format}\")\n",
    "        res.append((cd_format, response.json()))\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        logging.warning(f\"Failed to scrape data for date: {cd_format} (UTC)\\n\\tError: {http_err}\")\n",
    "    except requests.exceptions.JSONDecodeError as decoder_error:\n",
    "        logging.warning(f\"Failed to deserialize JSON: \\n\\tError: {decoder_error}\")\n",
    "    current_date -= delta\n",
    "\n",
    "\n",
    "print(len(res))\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Energy Mix Scraper"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "exclude_cross_boarder_e_trading = True\n",
    "cbet = \"Cross border electricity trading\"\n",
    "\n",
    "dtype = [('date', 'U50'), ('e_component', 'U50'), ('value', 'float32')]\n",
    "\n",
    "# Initialize an empty structured array\n",
    "array = np.empty(0, dtype=dtype)\n",
    "\n",
    "for date, data in res:\n",
    "    sources = []\n",
    "    for e_source in data:\n",
    "        name = str(e_source[\"name\"][\"en\"])\n",
    "\n",
    "        if exclude_cross_boarder_e_trading and name == cbet:\n",
    "            continue\n",
    "\n",
    "        # Ensure numeric conversion or default to 0\n",
    "        try:\n",
    "            y_value = float(e_source[\"y\"])\n",
    "        except (ValueError, TypeError):\n",
    "            continue\n",
    "        \n",
    "        sources.append((date, name, y_value))\n",
    "    \n",
    "    # Convert to a structured array with the correct dtype\n",
    "    arr = np.array(sources, dtype=dtype)\n",
    "    \n",
    "    # Normalize the 'value' column\n",
    "    arr['value'] /= np.sum(arr['value'], axis=0)\n",
    "\n",
    "    # Append to the main array\n",
    "    array = np.append(array, arr)\n",
    "\n",
    "np.savetxt(\"../data/daily_market_mix.csv\", array, delimiter=\",\", fmt=\"%s\")\n",
    "array"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "POST /api/raw-data HTTP/1.1\n",
    "Content-Type: application/json\n",
    "Accept: */*\n",
    "Sec-Fetch-Site: cross-site\n",
    "Accept-Language: en-GB,en;q=0.9\n",
    "Accept-Encoding: gzip, deflate, br\n",
    "Sec-Fetch-Mode: cors\n",
    "Origin: https://www.agora-energiewende.de\n",
    "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.0.1 Safari/605.1.15\n",
    "Content-Length: 538\n",
    "Referer: https://www.agora-energiewende.de/\n",
    "Connection: keep-alive\n",
    "Sec-Fetch-Dest: empty\n",
    "X-Requested-With: XMLHttpRequest\n",
    "Api-key: agora_live_62ce76dd202927.67115829\n",
    "Priority: u=3, i\n",
    "\n",
    "\n",
    "{\"filters\":{\"from\":\"2023-11-01\",\"to\":\"2024-10-01\",\"generation\":[\"Total electricity demand\",\"Biomass\",\"Hydro\",\"Wind offshore\",\"Wind onshore\",\"Solar\",\"Total conventional power plant\",\"Nuclear\",\"Lignite\",\"Hard Coal\",\"Natural Gas\",\"Pumped storage generation\",\"Other\",\"Grid emission factor\",\"Total grid emissions\",\"Total Renewables\",\"Total Conventional\",\"Renewable share\",\"Conventional share\"]},\"x_coordinate\":\"date_id\",\"y_coordinate\":\"value\",\"view_name\":\"live_gen_plus_emi_de_hourly\",\"kpi_name\":\"power_generation\",\"z_coordinate\":\"generation\"}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint and headers\n",
    "url = \"https://api.agora-energy.org/api/raw-data\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Sec-Fetch-Site\": \"cross-site\",\n",
    "    \"Accept-Language\": \"en-GB,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Sec-Fetch-Mode\": \"cors\",\n",
    "    \"Origin\": \"https://www.agora-energiewende.de\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/18.0.1 Safari/605.1.15\",\n",
    "    \"Referer\": \"https://www.agora-energiewende.de/\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Sec-Fetch-Dest\": \"empty\",\n",
    "    \"X-Requested-With\": \"XMLHttpRequest\",\n",
    "    \"Api-key\": \"agora_live_62ce76dd202927.67115829\",\n",
    "}\n",
    "\n",
    "out = []\n",
    "\n",
    "# Define the payload\n",
    "for year in [2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024]:\n",
    "    payload = {\n",
    "        \"filters\": {\n",
    "            \"from\": f\"{year}-10-01\",\n",
    "            \"to\": f\"{year + 1}-09-30\",\n",
    "            \"generation\": [\n",
    "                \"Total electricity demand\", \"Biomass\", \"Hydro\", \"Wind offshore\",\n",
    "                \"Wind onshore\", \"Solar\", \"Total conventional power plant\", \"Nuclear\",\n",
    "                \"Lignite\", \"Hard Coal\", \"Natural Gas\", \"Pumped storage generation\",\n",
    "                \"Other\", \"Grid emission factor\", \"Total grid emissions\", \"Total Renewables\",\n",
    "                \"Total Conventional\", \"Renewable share\", \"Conventional share\"\n",
    "            ]\n",
    "        },\n",
    "        \"x_coordinate\": \"date_id\",\n",
    "        \"y_coordinate\": \"value\",\n",
    "        \"view_name\": \"live_gen_plus_emi_de_hourly\",\n",
    "        \"kpi_name\": \"power_generation\",\n",
    "        \"z_coordinate\": \"generation\"\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "\n",
    "    # Check the response\n",
    "    if response.status_code == 200:\n",
    "        print(\"Request was successful!\", year, year+1)\n",
    "        data = {}\n",
    "        data = response.json()\n",
    "        out.extend(data[\"data\"][\"data\"])\n",
    "    else:\n",
    "        print(f\"Request failed with status code {response.status_code}\", year, year+1)\n",
    "    time.sleep(0.3)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "np.savetxt(\"../data/hourly_market_mix.csv\", np.array(out), delimiter=\",\", fmt=\"%s\")"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = np.array(out)\n",
    "\n",
    "mix_categories = [\n",
    "    \"Biomass\",\n",
    "    \"Hard Coal\",\n",
    "    \"Hydro\",\n",
    "    \"Lignite\",\n",
    "    \"Natural Gas\",\n",
    "    \"Nuclear\",\n",
    "    \"Other\",\n",
    "    \"Pumped storage generation\",\n",
    "    \"Solar\",\n",
    "    \"Wind offshore\",\n",
    "    \"Wind onshore\",\n",
    "]\n",
    "\n",
    "other_metrics = [\n",
    "    \"Grid emission factor\",\n",
    "    \"Total conventional power plant\",\n",
    "    \"Total electricity demand\",\n",
    "    \"Total grid emissions\",\n",
    "]\n",
    "\n",
    "# Define start and end dates as naive datetime objects\n",
    "start_date = datetime.fromisoformat(\"2018-10-01T00:00:00\")\n",
    "end_date = datetime.fromisoformat(\"2024-10-30T00:00:00\")\n",
    "\n",
    "# Generate hourly timestamps\n",
    "timestamps = [\n",
    "    start_date + timedelta(hours=i)\n",
    "    for i in range(int((end_date - start_date).total_seconds() // 3600) + 1)\n",
    "]\n",
    "timestamp_strings = [ts.isoformat() for ts in timestamps]\n",
    "mix_rows = []\n",
    "other_metrics_rows = []\n",
    "\n",
    "data_dict = {ts: [] for ts in timestamp_strings}\n",
    "for d in data:\n",
    "    d_timestamp = datetime.fromisoformat(d[0]).isoformat()  # Naive datetime conversion\n",
    "    if d_timestamp in data_dict:\n",
    "        data_dict[d_timestamp].append(d)\n",
    "for ts in timestamp_strings:\n",
    "    hour_data = np.array(data_dict.get(ts, []))  # Fetch data for this timestamp\n",
    "    if hour_data.size == 0:\n",
    "        continue\n",
    "\n",
    "    mix_per_hour = hour_data[np.isin(hour_data[:, 2], mix_categories)]\n",
    "    \n",
    "    if mix_per_hour.size == 0:\n",
    "        continue\n",
    "    \n",
    "    mix_per_hour = np.where(mix_per_hour == None, 0.0, mix_per_hour)\n",
    "\n",
    "\n",
    "    mix_per_hour[:, 1] = (\n",
    "        mix_per_hour[:, 1].astype(float) / mix_per_hour[:, 1].astype(float).sum()\n",
    "    )\n",
    "\n",
    "    row = np.concatenate(([ts], mix_per_hour[:, 1]))\n",
    "    if row.shape[0] == 11:\n",
    "        row = np.insert(row, 6, 0.0)\n",
    "    mix_rows.append(row)\n",
    "\n",
    "    other_metrics_per_hour = hour_data[np.isin(hour_data[:, 2], other_metrics)]\n",
    "    if other_metrics_per_hour.size == 0:\n",
    "        continue\n",
    "    row = np.concatenate(([ts], other_metrics_per_hour[:, 1]))\n",
    "    \n",
    "    other_metrics_rows.append(row)\n",
    "    \n",
    "percentage_mix = np.vstack([[\"Timestamp\"] + mix_categories] + mix_rows)\n",
    "percentage_sources = np.vstack([[\"Timestamp\"] + other_metrics] + other_metrics_rows)\n",
    "\n",
    "np.savetxt(\"../data/hourly_market_mix_cleaned.csv\", percentage_mix, delimiter=\",\", fmt=\"%s\")\n",
    "np.savetxt(\"../data/hourly_market_metrics_cleaned.csv\", percentage_sources, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Weather Data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "import importlib.util\n",
    "\n",
    "# Function to check if required packages are installed\n",
    "def check_package_installed(package_name):\n",
    "    package_spec = importlib.util.find_spec(package_name)\n",
    "    if package_spec is None:\n",
    "        print(f\"{package_name} is not installed!\")\n",
    "    else:\n",
    "        print(f\"{package_name} is installed!\")\n",
    "\n",
    "# Mapping of module names to package names\n",
    "packages = {\n",
    "    \"openmeteo_requests\": \"openmeteo-requests\",\n",
    "    \"requests_cache\": \"requests-cache\",\n",
    "    \"retry_requests\": \"retry-requests\",\n",
    "}\n",
    "\n",
    "for module_name, package_name in packages.items():\n",
    "    check_package_installed(module_name)\n",
    "\n",
    "# Setup the Open-Meteo API client with caching and retries\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Define the base URL for the weather API\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Define the list of representative coordinates for Germany\n",
    "coordinates = [\n",
    "    {\"latitude\": 52.52, \"longitude\": 13.405},  # Berlin\n",
    "    {\"latitude\": 53.5511, \"longitude\": 9.9937},  # Hamburg\n",
    "    {\"latitude\": 48.1351, \"longitude\": 11.5820},  # Munich\n",
    "    {\"latitude\": 50.9375, \"longitude\": 6.9603},  # Cologne\n",
    "    {\"latitude\": 50.1109, \"longitude\": 8.6821},  # Frankfurt\n",
    "    {\"latitude\": 51.0504, \"longitude\": 13.7373},  # Dresden\n",
    "    {\"latitude\": 48.7758, \"longitude\": 9.1829},  # Stuttgart\n",
    "]\n",
    "\n",
    "# Define the weather variables and date range\n",
    "params_template = {\n",
    "    \"start_date\": \"2018-01-01\",\n",
    "    \"end_date\": \"2024-11-21\",\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \n",
    "        \"surface_pressure\", \"cloud_cover\", \"wind_speed_100m\", \n",
    "        \"sunshine_duration\", \"shortwave_radiation\", \"direct_radiation\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Store data for all locations\n",
    "all_data = []\n",
    "\n",
    "for coord in coordinates:\n",
    "    params = params_template.copy()\n",
    "    params.update({\n",
    "        \"latitude\": coord[\"latitude\"],\n",
    "        \"longitude\": coord[\"longitude\"],\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        # Fetch weather data for the current location\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "\n",
    "        # Extract hourly data for this location\n",
    "        hourly = response.Hourly()\n",
    "        hourly_data = {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\"\n",
    "            )\n",
    "        }\n",
    "        hourly_data[\"temperature_2m\"] = hourly.Variables(0).ValuesAsNumpy()\n",
    "        hourly_data[\"relative_humidity_2m\"] = hourly.Variables(1).ValuesAsNumpy()\n",
    "        hourly_data[\"precipitation\"] = hourly.Variables(2).ValuesAsNumpy()\n",
    "        hourly_data[\"surface_pressure\"] = hourly.Variables(3).ValuesAsNumpy()\n",
    "        hourly_data[\"cloud_cover\"] = hourly.Variables(4).ValuesAsNumpy()\n",
    "        hourly_data[\"wind_speed_100m\"] = hourly.Variables(5).ValuesAsNumpy()\n",
    "        hourly_data[\"sunshine_duration\"] = hourly.Variables(6).ValuesAsNumpy()\n",
    "        hourly_data[\"shortwave_radiation\"] = hourly.Variables(7).ValuesAsNumpy()\n",
    "        hourly_data[\"direct_radiation\"] = hourly.Variables(8).ValuesAsNumpy()\n",
    "\n",
    "        # Convert to DataFrame and append to the list\n",
    "        hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "        all_data.append(hourly_dataframe)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for coordinates {coord}: {e}\")\n",
    "\n",
    "# Combine all data into one DataFrame\n",
    "combined_df = pd.concat(all_data)\n",
    "\n",
    "# Group by date and calculate the mean for all variables\n",
    "averaged_data = combined_df.groupby(\"date\").mean()\n",
    "\n",
    "# Rename columns for better understanding\n",
    "averaged_data = averaged_data.rename(columns={\n",
    "    \"shortwave_radiation\": \"Global Horizontal Irradiance\",\n",
    "    \"precipitation\": \"Precipitation (rain/snow)\"\n",
    "})\n",
    "\n",
    "# Save the averaged data to a CSV file\n",
    "csv_file = \"../data/germany_weather_average.csv\"\n",
    "averaged_data.to_csv(csv_file, index=True)\n",
    "\n",
    "print(f\"Averaged weather data for Germany saved to {csv_file}.\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Read CSV File\n",
    "csv_file = \"../data/germany_weather_average.csv\"\n",
    "data = pd.read_csv(csv_file)\n",
    "\n",
    "# filter all columns including numbers\n",
    "numeric_columns = data.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "# Find all columns with negativ values\n",
    "negative_columns = [col for col in numeric_columns if (data[col] < 0).any()]\n",
    "\n",
    "# Print all Headers with negativ values\n",
    "print(\"Columns with negative values:\", negative_columns)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"../data/germany_weather_average.csv\")\n",
    "\n",
    "non_numeric_columns = data.select_dtypes(exclude=[\"number\"]).columns\n",
    "\n",
    "print(\"Headers with non-numeric values:\", non_numeric_columns)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T18:40:33.104512Z",
     "start_time": "2024-12-18T18:40:19.140400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "from retry_requests import retry\n",
    "import importlib.util\n",
    "\n",
    "\n",
    "\n",
    "# Setup the Open-Meteo API client with caching and retries\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Define the base URL for the weather API\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Offshore wind parks (coordinates and weights)\n",
    "wind_parks = [\n",
    "    {\"latitude\": 54.008333, \"longitude\": 6.598333, \"weight\": 60},      # Alpha Ventus\n",
    "    {\"latitude\": 54.358333, \"longitude\": 5.975, \"weight\": 400},        # BARD Offshore I\n",
    "    {\"latitude\": 53.690, \"longitude\": 6.480, \"weight\": 113.4},         # Riffgat\n",
    "    {\"latitude\": 54.15, \"longitude\": 7.25, \"weight\": 295},             # Amrumbank West\n",
    "    {\"latitude\": 54.53, \"longitude\": 6.25, \"weight\": 200},             # Butendiek\n",
    "    {\"latitude\": 54.367, \"longitude\": 6.467, \"weight\": 295},           # DanTysk\n",
    "    {\"latitude\": 54.480, \"longitude\": 7.370, \"weight\": 288},           # Meerwind Süd|Ost\n",
    "    {\"latitude\": 54.4, \"longitude\": 6.6, \"weight\": 576},               # Gode Wind 1 & 2\n",
    "    {\"latitude\": 54.30, \"longitude\": 6.65, \"weight\": 400},             # Global Tech I\n",
    "    {\"latitude\": 53.88, \"longitude\": 6.59, \"weight\": 450},             # Borkum Riffgrund 1\n",
    "    {\"latitude\": 53.88, \"longitude\": 6.59, \"weight\": 215},             # Borkum Riffgrund 2\n",
    "    {\"latitude\": 54.00, \"longitude\": 6.58, \"weight\": 342},             # Trianel Windpark Borkum\n",
    "    {\"latitude\": 54.22, \"longitude\": 6.63, \"weight\": 332},             # Nordsee Ost\n",
    "    {\"latitude\": 54.25, \"longitude\": 7.25, \"weight\": 385},             # Hohe See\n",
    "    {\"latitude\": 54.28, \"longitude\": 7.30, \"weight\": 252},             # Albatros\n",
    "    {\"latitude\": 54.48, \"longitude\": 6.78, \"weight\": 350},             # Wikinger\n",
    "    {\"latitude\": 54.55, \"longitude\": 6.37, \"weight\": 402},             # Arkona\n",
    "    {\"latitude\": 54.45, \"longitude\": 6.58, \"weight\": 600},             # Veja Mate\n",
    "    {\"latitude\": 54.33, \"longitude\": 7.18, \"weight\": 300},             # Deutsche Bucht\n",
    "    {\"latitude\": 54.25, \"longitude\": 7.18, \"weight\": 402},             # Kaskasi\n",
    "]\n",
    "\n",
    "\n",
    "# Define the weather variable and date range\n",
    "params_template = {\n",
    "    \"start_date\": \"2018-01-01\",\n",
    "    \"end_date\": \"2024-11-21\",\n",
    "    \"hourly\": [\"wind_speed_100m\"]\n",
    "}\n",
    "\n",
    "# Store data for all locations\n",
    "weighted_wind_speed = []\n",
    "\n",
    "for park in wind_parks:\n",
    "    params = params_template.copy()\n",
    "    params.update({\n",
    "        \"latitude\": park[\"latitude\"],\n",
    "        \"longitude\": park[\"longitude\"],\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        # Fetch wind speed data for the current location\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "\n",
    "        # Extract hourly wind speed data for this location\n",
    "        hourly = response.Hourly()\n",
    "        hourly_data = {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\"\n",
    "            ),\n",
    "            \"wind_speed_100m\": hourly.Variables(0).ValuesAsNumpy()\n",
    "        }\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "\n",
    "        # Group by date and calculate the mean wind speed\n",
    "        daily_avg = hourly_dataframe.groupby(\"date\")[\"wind_speed_100m\"].mean()\n",
    "\n",
    "        # Weight the daily averages and append to the list\n",
    "        weighted_wind_speed.append(daily_avg * park[\"weight\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for wind park {park}: {e}\")\n",
    "\n",
    "# Combine weighted wind speeds across all parks\n",
    "total_weight = sum(park[\"weight\"] for park in wind_parks)\n",
    "combined_wind_speed = sum(weighted_wind_speed) / total_weight\n",
    "\n",
    "# Save the weighted average wind speed to a CSV file\n",
    "csv_file = \"../data/weighted_windspeed.csv\"\n",
    "combined_wind_speed.to_csv(csv_file, index=True, header=[\"windspeed 100m\"])\n",
    "\n",
    "print(f\"Weighted average wind speed saved to {csv_file}.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted average wind speed saved to ../data/weighted_windspeed.csv.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T18:47:26.224102Z",
     "start_time": "2024-12-18T18:47:24.914033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_orig = pd.read_csv('../data/germany_weather_average.csv')\n",
    "df_replacemet = pd.read_csv('../data/weighted_windspeed.csv')\n",
    "\n",
    "#replace column 1 from df_orig with column2 from df_replacement\n",
    "df_orig['wind_speed_100m'] = df_replacemet['windspeed 100m']\n",
    "\n",
    "#save new formatted csv\n",
    "df_orig.to_csv('../data/germany_weather_average.csv', index=False)\n",
    "\n",
    "#print(df_orig)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        38.490368\n",
      "1        39.208380\n",
      "2        39.968450\n",
      "3        41.720287\n",
      "4        43.592230\n",
      "           ...    \n",
      "60403    16.334919\n",
      "60404    13.902251\n",
      "60405    14.273662\n",
      "60406    14.276556\n",
      "60407    16.874899\n",
      "Name: wind_speed_100m, Length: 60408, dtype: float64\n",
      "0        52.056625\n",
      "1        54.795845\n",
      "2        61.359924\n",
      "3        65.063110\n",
      "4        58.501553\n",
      "           ...    \n",
      "60403    19.196121\n",
      "60404    14.000850\n",
      "60405    10.822694\n",
      "60406     6.807420\n",
      "60407     8.352756\n",
      "Name: windspeed 100m, Length: 60408, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Open-Meteo API Client und andere notwendige Bibliotheken importieren\n",
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import pandas as pd\n",
    "\n",
    "# Open-Meteo Setup\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=-1)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Base URL für API\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "\n",
    "# Parameter für die API-Abfrage\n",
    "params_template = {\n",
    "    \"start_date\": \"2018-01-01\",\n",
    "    \"end_date\": \"2024-11-21\",\n",
    "    \"hourly\": [\"sunshine_duration\", \"direct_radiation\", \"shortwave_radiation\"]\n",
    "}\n",
    "\n",
    "# Ergebnisse speichern\n",
    "all_data = []\n",
    "\n",
    "for park in solar_parks:\n",
    "    params = params_template.copy()\n",
    "    params.update({\n",
    "        \"latitude\": park[\"latitude\"],\n",
    "        \"longitude\": park[\"longitude\"]\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        # Daten abrufen\n",
    "        responses = openmeteo.weather_api(url, params=params)\n",
    "        response = responses[0]\n",
    "\n",
    "        # Stündliche Daten\n",
    "        hourly = response.Hourly()\n",
    "        hourly_data = {\n",
    "            \"date\": pd.date_range(\n",
    "                start=pd.to_datetime(hourly.Time(), unit=\"s\", utc=True),\n",
    "                end=pd.to_datetime(hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "                freq=pd.Timedelta(seconds=hourly.Interval()),\n",
    "                inclusive=\"left\"\n",
    "            )\n",
    "        }\n",
    "        hourly_data[\"sunshine_duration\"] = hourly.Variables(0).ValuesAsNumpy()\n",
    "        hourly_data[\"direct_radiation\"] = hourly.Variables(1).ValuesAsNumpy()\n",
    "        hourly_data[\"shortwave_radiation\"] = hourly.Variables(2).ValuesAsNumpy()\n",
    "\n",
    "        # In DataFrame konvertieren\n",
    "        hourly_dataframe = pd.DataFrame(data=hourly_data)\n",
    "        hourly_dataframe[\"weight\"] = park[\"weight\"]\n",
    "        all_data.append(hourly_dataframe)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler bei {park}: {e}\")\n",
    "\n",
    "# Kombiniere alle Daten\n",
    "combined_df = pd.concat(all_data)\n",
    "\n",
    "# Gruppiere nach Datum, berechne gewichteten Durchschnitt\n",
    "weighted_avg = (\n",
    "    combined_df.groupby(\"date\")\n",
    "    .apply(lambda x: pd.Series({\n",
    "        \"sunshine_duration\": (x[\"sunshine_duration\"] * x[\"weight\"]).sum() / x[\"weight\"].sum(),\n",
    "        \"direct_radiation\": (x[\"direct_radiation\"] * x[\"weight\"]).sum() / x[\"weight\"].sum(),\n",
    "        \"shortwave_radiation\": (x[\"shortwave_radiation\"] * x[\"weight\"]).sum() / x[\"weight\"].sum(),\n",
    "    }))\n",
    ")\n",
    "\n",
    "# Speichere Daten in CSV\n",
    "output_file = \"../data/solar_park_weather_average.csv\"\n",
    "weighted_avg.to_csv(output_file, index=True)\n",
    "\n",
    "print(f\"Gewichtete Wetterdaten gespeichert unter {output_file}.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pj-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
