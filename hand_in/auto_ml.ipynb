{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import xgboost as xgb\n",
    "# import optuna\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# from src.dataset import get_datasets\n",
    "# from src.feature import create_features, split\n",
    "# # Adjust the import below to match your project structure.\n",
    "# from src.benchmark import extend_by_predictions_and_samples\n",
    "\n",
    "# # Prepare the data for checking candidate features.\n",
    "# data = create_features(get_datasets()[0])\n",
    "# data.index = pd.to_datetime(data.index)\n",
    "# data['day'] = data.index.date\n",
    "\n",
    "# # Define the candidate features.\n",
    "# candidate_features = [\n",
    "#     'Hydro',\n",
    "#     'Pumped storage generation', 'Solar',\n",
    "#     'Wind offshore', 'Wind onshore', 'temperature_2m', 'precipitation',\n",
    "#     'wind_speed_100m', 'direct_radiation', 'hour', 'dayofweek', 'dayofyear'\n",
    "# ]\n",
    "\n",
    "# for i in range(3, 15, 2):\n",
    "#     candidate_features.append(f'ma_{i}_days_pumped_storage_generation')\n",
    "# candidate_features.append(f'ma_2_days_pumped_storage_generation')\n",
    "# candidate_features.append(f'ma_14_days_pumped_storage_generation')\n",
    "\n",
    "# def objective(trial):\n",
    "#     # --------------------------------------------------\n",
    "#     # 1. Feature Selection\n",
    "#     # --------------------------------------------------\n",
    "#     # Let Optuna decide whether to include each candidate feature.\n",
    "#     selected_features = []\n",
    "#     for feature in candidate_features:\n",
    "#         use_feature = trial.suggest_categorical(feature, [True, False])\n",
    "#         if use_feature and feature in data.columns:\n",
    "#             selected_features.append(feature)\n",
    "#     # If no feature was selected, fall back to using all candidate features.\n",
    "#     if len(selected_features) == 0:\n",
    "#         selected_features = candidate_features\n",
    "\n",
    "#     # --------------------------------------------------\n",
    "#     # 2. Hyperparameter Search Space for XGBoost\n",
    "#     # --------------------------------------------------\n",
    "#     # Sample early_stopping_rounds separately so it can be passed to fit.\n",
    "#     params = {\n",
    "#         # 'base_score': trial.suggest_float('base_score', 0.0, 1.0),\n",
    "#         # 'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.5, log=True),\n",
    "#         # 'max_depth': trial.suggest_int('max_depth', 3, 100),\n",
    "#         # 'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "#         # 'gamma': trial.suggest_float('gamma', 0, 5.0),\n",
    "#         # 'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "#         # 'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "#         # 'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n",
    "#         # 'max_delta_step': trial.suggest_int('max_delta_step', 0, 10),\n",
    "#         # 'early_stopping_rounds': trial.suggest_int('early_stopping_rounds', 10, 250)\n",
    "#     }\n",
    "\n",
    "#     # --------------------------------------------------\n",
    "#     # 3. Rolling Forecast Loop with Early Stopping\n",
    "#     # --------------------------------------------------\n",
    "#     merged_df, _ = get_datasets()\n",
    "#     # The split function is assumed to return (train, eval, test, benchmark, …)\n",
    "#     train_df, eval_df, test_df, benchmark_df, _, _, _ = split(merged_df)\n",
    "\n",
    "#     # Combine available training data.\n",
    "#     training_set = pd.concat([train_df, eval_df, test_df])\n",
    "#     # Benchmarking set for rolling forecast.\n",
    "#     benchmarking_set = benchmark_df\n",
    "\n",
    "#     # Ensure the datasets are sorted by their datetime index.\n",
    "#     training_set = training_set.sort_index()\n",
    "#     benchmarking_set = benchmarking_set.sort_index()\n",
    "\n",
    "#     WINDOW_SIZE = 24  # Forecast window size (24 hours)\n",
    "#     rmse_scores = []\n",
    "\n",
    "#     # Loop over forecast windows (starting at index 24 to ensure an initial training period).\n",
    "#     for window_start in range(24, len(benchmarking_set) - WINDOW_SIZE, WINDOW_SIZE):\n",
    "#         # The actual target values for the current forecast window.\n",
    "#         y_actual = benchmarking_set.iloc[window_start: window_start + WINDOW_SIZE][\"Price\"]\n",
    "\n",
    "#         # Extend the dataset with training data and all benchmark data up to the current window.\n",
    "#         dataset_extended = pd.concat([training_set, benchmarking_set.iloc[:window_start]])\n",
    "#         next_timestamp = dataset_extended.index[-1] + pd.DateOffset(hours=1)\n",
    "#         if next_timestamp != y_actual.index[0]:\n",
    "#             print(f\"\\nSkipping prediction for {next_timestamp} due to missing entries.\\n\"\n",
    "#                   \"--------------------------------------------------------------\")\n",
    "#             continue\n",
    "\n",
    "#         # Optionally extend the dataset with predictions/samples.\n",
    "#         dataset_extended_ps = extend_by_predictions_and_samples(\n",
    "#             dataset_extended, dataset_extended.index[-1]\n",
    "#         )\n",
    "\n",
    "#         # Create features on the extended dataset.\n",
    "#         dataset_extended_features = create_features(dataset_extended_ps)\n",
    "\n",
    "#         # Check that the forecast window rows are present.\n",
    "#         if not set(y_actual.index).issubset(dataset_extended_features.index):\n",
    "#             print(\"Forecast window indices are missing in features, skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         # Split the extended dataset into training and forecast portions.\n",
    "#         # Training data: all data except the last WINDOW_SIZE hours.\n",
    "#         train_data = dataset_extended_features.iloc[:-WINDOW_SIZE]\n",
    "#         # Create an evaluation set (the latter 20% of the training data) for early stopping.\n",
    "#         split_idx = int(0.8 * len(train_data))\n",
    "#         X_train = train_data.iloc[:split_idx][selected_features]\n",
    "#         y_train = train_data.iloc[:split_idx][\"Price\"]\n",
    "#         X_eval = train_data.iloc[split_idx:][selected_features]\n",
    "#         y_eval = train_data.iloc[split_idx:][\"Price\"]\n",
    "\n",
    "#         # The prediction set is the forecast window.\n",
    "#         X_predict = dataset_extended_features.reindex(y_actual.index)[selected_features]\n",
    "#         if X_predict.isnull().any().any():\n",
    "#             print(\"Missing features in forecast window, skipping.\")\n",
    "#             continue\n",
    "\n",
    "#         model = xgb.XGBRegressor(\n",
    "#             **params,\n",
    "#             objective='reg:squarederror',\n",
    "#             eval_metric='rmse',\n",
    "#             random_state=42,\n",
    "#             n_jobs=-1\n",
    "#         )\n",
    "\n",
    "#         model.fit(\n",
    "#             X_train, y_train,\n",
    "#             eval_set=[(X_eval, y_eval)],\n",
    "#             verbose=False\n",
    "#         )\n",
    "\n",
    "#         preds = model.predict(X_predict)\n",
    "#         rmse = mean_squared_error(y_actual, preds)  ** 0.5\n",
    "#         rmse_scores.append(rmse)\n",
    "\n",
    "#     if len(rmse_scores) == 0:\n",
    "#         return float('inf')\n",
    "#     avg_rmse = np.mean(rmse_scores)\n",
    "#     return avg_rmse\n",
    "\n",
    "# # =============================================================================\n",
    "# # Run the Optuna Study\n",
    "# # =============================================================================\n",
    "# n_trials = 300\n",
    "# pbar = tqdm(total=n_trials, desc=\"Optuna Trials\")\n",
    "\n",
    "# def progress_callback(study, trial):\n",
    "#     pbar.update(1)\n",
    "\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=n_trials, n_jobs=-1, callbacks=[progress_callback])\n",
    "\n",
    "# print(\"Best trial:\")\n",
    "# best_trial = study.best_trial\n",
    "# print(f\"  Average RMSE: {best_trial.value}\")\n",
    "# print(\"  Best hyperparameters:\")\n",
    "# for key, value in best_trial.params.items():\n",
    "#     print(f\"    {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fc3fe5c50747d895aac73cf36da3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optuna Trials:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-09 21:15:09,307] A new study created in memory with name: no-name-93fde0cf-9498-45d1-a520-308e9a044638\n",
      "21:15:41 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:15:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:16:43 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:16:58 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:18 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:20 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:52 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:17:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:17:53 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:13 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:21 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:18:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:18:52 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:00 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:20 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:19:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:40 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:51 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:19:57 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:06 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:17 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:29 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:20:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:20:54 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:21:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:18 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:21:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:42 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:21:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:01 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:02 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:06 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:07 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:16 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:19 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:28 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:22:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:22:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:23:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:15 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:23:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:23:57 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 183\u001b[0m\n\u001b[1;32m    180\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    182\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 183\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mprogress_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m pbar\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/optuna/study/_optimize.py:102\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     99\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m completed:\n\u001b[1;32m    100\u001b[0m                         f\u001b[38;5;241m.\u001b[39mresult()\n\u001b[0;32m--> 102\u001b[0m                 futures\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    103\u001b[0m                     executor\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    104\u001b[0m                         _optimize_sequential,\n\u001b[1;32m    105\u001b[0m                         study,\n\u001b[1;32m    106\u001b[0m                         func,\n\u001b[1;32m    107\u001b[0m                         \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    108\u001b[0m                         timeout,\n\u001b[1;32m    109\u001b[0m                         catch,\n\u001b[1;32m    110\u001b[0m                         callbacks,\n\u001b[1;32m    111\u001b[0m                         gc_after_trial,\n\u001b[1;32m    112\u001b[0m                         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    113\u001b[0m                         time_start,\n\u001b[1;32m    114\u001b[0m                         progress_bar,\n\u001b[1;32m    115\u001b[0m                     )\n\u001b[1;32m    116\u001b[0m                 )\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     study\u001b[38;5;241m.\u001b[39m_thread_local\u001b[38;5;241m.\u001b[39min_optimize_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/_base.py:636\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 636\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/concurrent/futures/thread.py:229\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 229\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:1053\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1053\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/threading.py:1069\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lock \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# already determined that the C code is done\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[0;32m-> 1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1070\u001b[0m     lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:24:01 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:02 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:03 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:04 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:13 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:14 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:22 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:24:26 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:42 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:49 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:24:50 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:08 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:12 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:17 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:23 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:25:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:43 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:51 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:25:56 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "21:26:07 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "21:26:13 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from prophet import Prophet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.dataset import get_datasets\n",
    "from src.feature import create_features, split\n",
    "from src.benchmark import extend_by_predictions_and_samples\n",
    "\n",
    "# =============================================================================\n",
    "# Prepare the Data and Candidate Features\n",
    "# =============================================================================\n",
    "\n",
    "# Use an initial dataset to check available features.\n",
    "initial_dataset = get_datasets()[0]\n",
    "data = create_features(initial_dataset)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "data['day'] = data.index.date\n",
    "\n",
    "# Define candidate features.\n",
    "candidate_features = [\n",
    "    'Hydro',\n",
    "    'Pumped storage generation',\n",
    "    'Solar',\n",
    "    'Wind offshore',\n",
    "    'Wind onshore',\n",
    "    'temperature_2m',\n",
    "    'precipitation',\n",
    "    'wind_speed_100m',\n",
    "    'direct_radiation',\n",
    "    'hour',\n",
    "    'dayofweek',\n",
    "    'dayofyear'\n",
    "]\n",
    "\n",
    "# Append moving-average features for various windows.\n",
    "for i in range(3, 15, 2):\n",
    "    candidate_features.append(f'ma_{i}_days_pumped_storage_generation')\n",
    "\n",
    "# Append fixed moving-average features (only once).\n",
    "candidate_features.append('ma_2_days_pumped_storage_generation')\n",
    "candidate_features.append('ma_14_days_pumped_storage_generation')\n",
    "\n",
    "# Remove duplicates (if any) while preserving order.\n",
    "candidate_features = list(dict.fromkeys(candidate_features))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Objective Function for Optuna\n",
    "# =============================================================================\n",
    "def objective(trial):\n",
    "    # --------------------------------------------------\n",
    "    # 1. Feature Selection\n",
    "    # --------------------------------------------------\n",
    "    # Let Optuna decide whether to include each candidate feature.\n",
    "    selected_features = []\n",
    "    for feature in candidate_features:\n",
    "        use_feature = trial.suggest_categorical(feature, [True, False])\n",
    "        if use_feature and feature in data.columns:\n",
    "            selected_features.append(feature)\n",
    "    # If no feature was selected, fall back to using all candidate features.\n",
    "    if len(selected_features) == 0:\n",
    "        selected_features = candidate_features\n",
    "    # Remove any duplicates.\n",
    "    selected_features = list(dict.fromkeys(selected_features))\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 2. Hyperparameter Search Space for Prophet (Optional)\n",
    "    # --------------------------------------------------\n",
    "    # Uncomment the lines below if you want to tune these parameters.\n",
    "    # changepoint_prior_scale = trial.suggest_float('changepoint_prior_scale', 0.001, 0.5, log=True)\n",
    "    # seasonality_prior_scale = trial.suggest_float('seasonality_prior_scale', 1.0, 20.0, log=True)\n",
    "    # holidays_prior_scale    = trial.suggest_float('holidays_prior_scale', 1.0, 20.0, log=True)\n",
    "    # seasonality_mode        = trial.suggest_categorical('seasonality_mode', ['additive', 'multiplicative'])\n",
    "    \n",
    "    # --------------------------------------------------\n",
    "    # 3. Rolling Forecast Loop with Early Stopping\n",
    "    # --------------------------------------------------\n",
    "    merged_df, _ = get_datasets()\n",
    "    # The split function is assumed to return (train, eval, test, benchmark, …)\n",
    "    train_df, eval_df, test_df, benchmark_df, _, _, _ = split(merged_df)\n",
    "\n",
    "    # Combine available training data.\n",
    "    training_set = pd.concat([train_df, eval_df, test_df])\n",
    "    benchmarking_set = benchmark_df\n",
    "\n",
    "    # Ensure the datasets are sorted by their datetime index.\n",
    "    training_set = training_set.sort_index()\n",
    "    benchmarking_set = benchmarking_set.sort_index()\n",
    "\n",
    "    WINDOW_SIZE = 24  # Forecast window size (24 hours)\n",
    "    rmse_scores = []\n",
    "\n",
    "    # Loop over forecast windows (starting at index 24 to ensure an initial training period).\n",
    "    for window_start in range(24, len(benchmarking_set) - WINDOW_SIZE, WINDOW_SIZE):\n",
    "        # The actual target values for the current forecast window.\n",
    "        y_actual = benchmarking_set.iloc[window_start: window_start + WINDOW_SIZE][\"Price\"]\n",
    "\n",
    "        # Extend the dataset with training data and benchmark data up to the current window.\n",
    "        dataset_extended = pd.concat([training_set, benchmarking_set.iloc[:window_start]])\n",
    "        next_timestamp = dataset_extended.index[-1] + pd.DateOffset(hours=1)\n",
    "        if next_timestamp != y_actual.index[0]:\n",
    "            print(f\"\\nSkipping prediction for {next_timestamp} due to missing entries.\\n\"\n",
    "                  \"--------------------------------------------------------------\")\n",
    "            continue\n",
    "\n",
    "        # Optionally extend the dataset with predictions/samples.\n",
    "        dataset_extended_ps = extend_by_predictions_and_samples(\n",
    "            dataset_extended, dataset_extended.index[-1]\n",
    "        )\n",
    "\n",
    "        # Create features on the extended dataset.\n",
    "        dataset_extended_features = create_features(dataset_extended_ps)\n",
    "\n",
    "        # Ensure forecast window rows are present in the features.\n",
    "        if not set(y_actual.index).issubset(dataset_extended_features.index):\n",
    "            print(\"Forecast window indices are missing in features, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Split for training (all but last 24 hours) and prediction (the next 24 hours)\n",
    "        X_train_df = dataset_extended_features.iloc[:-WINDOW_SIZE]\n",
    "        try:\n",
    "            # Use .loc to select the forecast window rows.\n",
    "            X_predict_df = dataset_extended_features.loc[y_actual.index, selected_features]\n",
    "        except KeyError as e:\n",
    "            print(\"Error selecting forecast features:\", e)\n",
    "            continue\n",
    "\n",
    "        if X_predict_df.isnull().any().any():\n",
    "            print(\"Missing features in forecast window, skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Prepare Prophet-compatible training dataframe.\n",
    "        X_train_prophet = (\n",
    "            X_train_df[[\"Price\"] + selected_features]\n",
    "            .rename(columns={\"Price\": \"y\"})\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\": \"ds\"})\n",
    "            .dropna()\n",
    "        )\n",
    "        prophet_X_predict = X_predict_df.reset_index().rename(columns={\"index\": \"ds\"})\n",
    "\n",
    "        # Initialize and fit the Prophet model.\n",
    "        model = Prophet(\n",
    "            # Uncomment the lines below if you want to pass tuned hyperparameters:\n",
    "            # changepoint_prior_scale=changepoint_prior_scale,\n",
    "            # seasonality_prior_scale=seasonality_prior_scale,\n",
    "            # holidays_prior_scale=holidays_prior_scale,\n",
    "            # seasonality_mode=seasonality_mode\n",
    "        )\n",
    "       \n",
    "        # Add selected regressors.\n",
    "        for reg in selected_features:\n",
    "            model.add_regressor(reg)\n",
    "\n",
    "        model.fit(X_train_prophet)\n",
    "       \n",
    "        # Generate a forecast.\n",
    "        forecast = model.predict(prophet_X_predict)\n",
    "        prophet_forecast = forecast[\"yhat\"]\n",
    "        rmse = mean_squared_error(y_actual, prophet_forecast) ** 0.5\n",
    "        rmse_scores.append(rmse)\n",
    "\n",
    "    if len(rmse_scores) == 0:\n",
    "        return float('inf')\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    return avg_rmse\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Run the Optuna Study\n",
    "# =============================================================================\n",
    "n_trials = 100\n",
    "pbar = tqdm(total=n_trials, desc=\"Optuna Trials\")\n",
    "\n",
    "def progress_callback(study, trial):\n",
    "    pbar.update(1)\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=n_trials, n_jobs=-1, callbacks=[progress_callback])\n",
    "pbar.close()\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  Average RMSE: {best_trial.value}\")\n",
    "print(\"  Best hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
